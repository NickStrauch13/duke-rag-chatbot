{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommymmcguire/Duke-AI-MEng-chatbot/blob/main/notebooks/finetune_mistral_instruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWzCJYzdfQdY"
      },
      "source": [
        "## Setup Lit GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3FKcu3ZMqwS",
        "outputId": "73d57891-8619-48d6-b5d3-166cb879085e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting litgpt[all]@ git+https://github.com/Lightning-AI/litgpt\n",
            "  Cloning https://github.com/Lightning-AI/litgpt to /tmp/pip-install-cx81yzpa/litgpt_b9e5b57fed394e2db7f4d3976a262236\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Lightning-AI/litgpt /tmp/pip-install-cx81yzpa/litgpt_b9e5b57fed394e2db7f4d3976a262236\n",
            "  Resolved https://github.com/Lightning-AI/litgpt to commit 3516beacd3ff2ccca51fdc6e2471ff5adcc35e4f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.2.1+cu121)\n",
            "Collecting lightning==2.3.0.dev20240328 (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading lightning-2.3.0.dev20240328-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonargparse[signatures]>=4.27.6 (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading jsonargparse-4.27.7-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.2/192.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.42.0 (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.1.99)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.15.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.31.0)\n",
            "Collecting litdata (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading litdata-0.2.3-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zstandard (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.0.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (14.0.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.15.2)\n",
            "Collecting torchmetrics (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (4.38.2)\n",
            "Collecting lm-eval>=0.4.2 (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading lm_eval-0.4.2-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.4.2)\n",
            "Collecting huggingface-hub[hf_transfer]>=0.21.0 (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.42.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.11.4)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning==2.3.0.dev20240328->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.3.0.dev20240328->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2023.6.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning==2.3.0.dev20240328->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning==2.3.0.dev20240328->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.25.2)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.3.0.dev20240328->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (24.0)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.3.0.dev20240328->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.3.0.dev20240328->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (4.11.0)\n",
            "Collecting pytorch-lightning (from lightning==2.3.0.dev20240328->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading pytorch_lightning-2.2.2-py3-none-any.whl (801 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[hf_transfer]>=0.21.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.13.4)\n",
            "Collecting hf-transfer>=0.1.4 (from huggingface-hub[hf_transfer]>=0.21.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from jsonargparse[signatures]>=4.27.6->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.16)\n",
            "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]>=4.27.6->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading typeshed_client-2.5.1-py3-none-any.whl (606 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.1/606.1 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.21.0 (from lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading accelerate-0.29.2-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate (from lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonlines (from lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.10.0)\n",
            "Collecting peft>=0.2.0 (from lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11>=2.6.2 (from lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.0/235.0 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytablewriter (from lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score>=0.0.4 (from lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu>=1.5.0 (from lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading sacrebleu-2.4.1-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.2.2)\n",
            "Collecting sqlitedict (from lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tqdm-multiprocess (from lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Collecting dill (from lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting word2number (from lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (10.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.6)\n",
            "Collecting xxhash (from datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2023.12.25)\n",
            "Collecting boto3[crt] (from litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading boto3-1.34.83-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2024.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.62.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (5.9.5)\n",
            "Collecting responses<0.19 (from evaluate->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (4.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.3.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.8.1)\n",
            "Collecting portalocker (from sacrebleu>=1.5.0->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.5.0->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (4.9.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]>=4.27.6->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.1.5)\n",
            "Collecting botocore<1.35.0,>=1.34.83 (from boto3[crt]->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading botocore-1.34.83-py3-none-any.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3[crt]->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3[crt]->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting DataProperty<2,>=1.0.1 (from pytablewriter->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading pathvalidate-3.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading tcolorpy-0.1.4-py3-none-any.whl (7.9 kB)\n",
            "Collecting typepy[datetime]<2,>=1.3.2 (from pytablewriter->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.3.0)\n",
            "Collecting awscrt==0.19.19 (from botocore<1.35.0,>=1.34.83->boto3[crt]->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading awscrt-0.19.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (5.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score>=0.0.4->lm-eval>=0.4.2->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (8.1.7)\n",
            "Building wheels for collected packages: litgpt, rouge-score, sqlitedict, word2number\n",
            "  Building wheel for litgpt (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for litgpt: filename=litgpt-0.3.0.dev0-py3-none-any.whl size=141850 sha256=28c5419bea66c27b119a93105d543b3bef8170e7e7278a079ef921911dcf664f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kg2zjgrz/wheels/e1/35/84/dd27bf19233f92182da0cb9b138645406f0c7a780128f1beab\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=290df8ef72e46b75c5426cbf7cce435af5c858ecc2c980493af12fa887fe6de9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=f0db17cf589adc53feb949aa76c9df7ab1b86002cfa97e416ec1f6b8658709c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5566 sha256=fb70cd0ea9debb1aa57d1b1e1fad260b1dbb29db56ae65582166973ae5f0acf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
            "Successfully built litgpt rouge-score sqlitedict word2number\n",
            "Installing collected packages: word2number, sqlitedict, zstandard, xxhash, typeshed-client, tcolorpy, pybind11, portalocker, pathvalidate, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mbstrdecoder, lightning-utilities, jsonlines, jsonargparse, jmespath, hf-transfer, dill, colorama, awscrt, typepy, tqdm-multiprocess, sacrebleu, rouge-score, responses, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, botocore, bitsandbytes, s3transfer, nvidia-cusolver-cu12, datasets, DataProperty, boto3, torchmetrics, tabledata, evaluate, accelerate, pytorch-lightning, pytablewriter, peft, litdata, lm-eval, lightning, litgpt\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed DataProperty-1.0.1 accelerate-0.29.2 awscrt-0.19.19 bitsandbytes-0.42.0 boto3-1.34.83 botocore-1.34.83 colorama-0.4.6 datasets-2.18.0 dill-0.3.8 evaluate-0.4.1 hf-transfer-0.1.6 huggingface-hub-0.22.2 jmespath-1.0.1 jsonargparse-4.27.7 jsonlines-4.0.0 lightning-2.3.0.dev20240328 lightning-utilities-0.11.2 litdata-0.2.3 litgpt-0.3.0.dev0 lm-eval-0.4.2 mbstrdecoder-1.1.3 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pathvalidate-3.2.0 peft-0.10.0 portalocker-2.8.2 pybind11-2.12.0 pytablewriter-1.2.0 pytorch-lightning-2.2.2 responses-0.18.0 rouge-score-0.1.2 s3transfer-0.10.1 sacrebleu-2.4.1 sqlitedict-2.1.0 tabledata-1.3.3 tcolorpy-0.1.4 torchmetrics-1.3.2 tqdm-multiprocess-0.0.11 typepy-1.3.2 typeshed-client-2.5.1 word2number-1.1 xxhash-3.4.1 zstandard-0.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install 'litgpt[all] @ git+https://github.com/Lightning-AI/litgpt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42B2BYEGfGip",
        "outputId": "5425477e-4129-41a1-e531-ff2a8075bb91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1hOmj2wgrhA"
      },
      "source": [
        "### Download Model - Mistral7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyc47J5tfNNO",
        "outputId": "74b3dcb6-ef5f-4706-eb24-8502939b215d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n",
            "config.json: 100% 571/571 [00:00<00:00, 3.95MB/s]\n",
            "generation_config.json: 100% 116/116 [00:00<00:00, 593kB/s]\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.94G/9.94G [00:30<00:00, 324MB/s]\n",
            "pytorch_model-00002-of-00002.bin: 100% 5.06G/5.06G [00:16<00:00, 313MB/s]\n",
            "pytorch_model.bin.index.json: 100% 23.9k/23.9k [00:00<00:00, 80.2MB/s]\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 4.37MB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 452MB/s]\n",
            "tokenizer_config.json: 100% 967/967 [00:00<00:00, 5.64MB/s]\n",
            "Converting checkpoint files to LitGPT format.\n",
            "Processing checkpoints/mistralai/Mistral-7B-v0.1/pytorch_model-00001-of-00002.bin\n",
            "Loading 'model.embed_tokens.weight' into RAM\n",
            "Loading 'model.layers.0.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.0.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.0.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.0.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.0.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.0.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.1.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.1.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.1.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.1.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.1.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.1.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.2.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.2.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.2.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.2.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.2.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.2.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.3.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.3.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.3.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.3.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.3.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.3.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.4.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.4.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.4.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.4.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.4.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.4.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.5.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.5.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.5.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.5.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.5.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.5.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.6.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.6.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.6.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.6.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.6.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.6.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.7.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.7.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.7.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.7.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.7.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.7.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.8.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.8.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.8.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.8.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.8.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.8.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.9.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.9.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.9.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.9.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.9.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.9.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.10.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.10.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.10.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.10.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.10.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.10.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.11.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.11.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.11.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.11.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.11.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.11.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.12.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.12.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.12.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.12.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.12.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.12.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.13.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.13.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.13.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.13.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.13.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.13.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.14.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.14.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.14.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.14.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.14.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.14.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.15.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.15.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.15.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.15.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.15.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.15.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.16.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.16.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.16.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.16.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.16.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.16.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.17.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.17.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.17.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.17.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.17.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.17.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.18.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.18.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.18.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.18.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.18.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.18.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.19.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.19.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.19.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.19.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.19.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.19.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.20.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.20.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.20.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.20.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.20.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.20.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.21.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.21.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.21.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.21.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.21.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.21.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.22.self_attn.o_proj.weight' into RAM\n",
            "Loading 'layer 0 q' into RAM\n",
            "Loading 'layer 0 k' into RAM\n",
            "Loading 'layer 0 v' into RAM\n",
            "Loading 'layer 1 q' into RAM\n",
            "Loading 'layer 1 k' into RAM\n",
            "Loading 'layer 1 v' into RAM\n",
            "Loading 'layer 2 q' into RAM\n",
            "Loading 'layer 2 k' into RAM\n",
            "Loading 'layer 2 v' into RAM\n",
            "Loading 'layer 3 q' into RAM\n",
            "Loading 'layer 3 k' into RAM\n",
            "Loading 'layer 3 v' into RAM\n",
            "Loading 'layer 4 q' into RAM\n",
            "Loading 'layer 4 k' into RAM\n",
            "Loading 'layer 4 v' into RAM\n",
            "Loading 'layer 5 q' into RAM\n",
            "Loading 'layer 5 k' into RAM\n",
            "Loading 'layer 5 v' into RAM\n",
            "Loading 'layer 6 q' into RAM\n",
            "Loading 'layer 6 k' into RAM\n",
            "Loading 'layer 6 v' into RAM\n",
            "Loading 'layer 7 q' into RAM\n",
            "Loading 'layer 7 k' into RAM\n",
            "Loading 'layer 7 v' into RAM\n",
            "Loading 'layer 8 q' into RAM\n",
            "Loading 'layer 8 k' into RAM\n",
            "Loading 'layer 8 v' into RAM\n",
            "Loading 'layer 9 q' into RAM\n",
            "Loading 'layer 9 k' into RAM\n",
            "Loading 'layer 9 v' into RAM\n",
            "Loading 'layer 10 q' into RAM\n",
            "Loading 'layer 10 k' into RAM\n",
            "Loading 'layer 10 v' into RAM\n",
            "Loading 'layer 11 q' into RAM\n",
            "Loading 'layer 11 k' into RAM\n",
            "Loading 'layer 11 v' into RAM\n",
            "Loading 'layer 12 q' into RAM\n",
            "Loading 'layer 12 k' into RAM\n",
            "Loading 'layer 12 v' into RAM\n",
            "Loading 'layer 13 q' into RAM\n",
            "Loading 'layer 13 k' into RAM\n",
            "Loading 'layer 13 v' into RAM\n",
            "Loading 'layer 14 q' into RAM\n",
            "Loading 'layer 14 k' into RAM\n",
            "Loading 'layer 14 v' into RAM\n",
            "Loading 'layer 15 q' into RAM\n",
            "Loading 'layer 15 k' into RAM\n",
            "Loading 'layer 15 v' into RAM\n",
            "Loading 'layer 16 q' into RAM\n",
            "Loading 'layer 16 k' into RAM\n",
            "Loading 'layer 16 v' into RAM\n",
            "Loading 'layer 17 q' into RAM\n",
            "Loading 'layer 17 k' into RAM\n",
            "Loading 'layer 17 v' into RAM\n",
            "Loading 'layer 18 q' into RAM\n",
            "Loading 'layer 18 k' into RAM\n",
            "Loading 'layer 18 v' into RAM\n",
            "Loading 'layer 19 q' into RAM\n",
            "Loading 'layer 19 k' into RAM\n",
            "Loading 'layer 19 v' into RAM\n",
            "Loading 'layer 20 q' into RAM\n",
            "Loading 'layer 20 k' into RAM\n",
            "Loading 'layer 20 v' into RAM\n",
            "Loading 'layer 21 q' into RAM\n",
            "Loading 'layer 21 k' into RAM\n",
            "Loading 'layer 21 v' into RAM\n",
            "Loading 'layer 22 q' into RAM\n",
            "Loading 'layer 22 k' into RAM\n",
            "Loading 'layer 22 v' into RAM\n",
            "Processing checkpoints/mistralai/Mistral-7B-v0.1/pytorch_model-00002-of-00002.bin\n",
            "Loading 'model.layers.22.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.22.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.22.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.22.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.22.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.23.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.23.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.23.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.23.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.23.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.23.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.24.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.24.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.24.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.24.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.24.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.24.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.25.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.25.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.25.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.25.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.25.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.25.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.26.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.26.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.26.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.26.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.26.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.26.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.27.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.27.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.27.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.27.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.27.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.27.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.28.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.28.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.28.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.28.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.28.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.28.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.29.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.29.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.29.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.29.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.29.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.29.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.30.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.30.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.30.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.30.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.30.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.30.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.31.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.31.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.31.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.31.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.31.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.31.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.norm.weight' into RAM\n",
            "Loading 'lm_head.weight' into RAM\n",
            "Loading 'layer 23 q' into RAM\n",
            "Loading 'layer 23 k' into RAM\n",
            "Loading 'layer 23 v' into RAM\n",
            "Loading 'layer 24 q' into RAM\n",
            "Loading 'layer 24 k' into RAM\n",
            "Loading 'layer 24 v' into RAM\n",
            "Loading 'layer 25 q' into RAM\n",
            "Loading 'layer 25 k' into RAM\n",
            "Loading 'layer 25 v' into RAM\n",
            "Loading 'layer 26 q' into RAM\n",
            "Loading 'layer 26 k' into RAM\n",
            "Loading 'layer 26 v' into RAM\n",
            "Loading 'layer 27 q' into RAM\n",
            "Loading 'layer 27 k' into RAM\n",
            "Loading 'layer 27 v' into RAM\n",
            "Loading 'layer 28 q' into RAM\n",
            "Loading 'layer 28 k' into RAM\n",
            "Loading 'layer 28 v' into RAM\n",
            "Loading 'layer 29 q' into RAM\n",
            "Loading 'layer 29 k' into RAM\n",
            "Loading 'layer 29 v' into RAM\n",
            "Loading 'layer 30 q' into RAM\n",
            "Loading 'layer 30 k' into RAM\n",
            "Loading 'layer 30 v' into RAM\n",
            "Loading 'layer 31 q' into RAM\n",
            "Loading 'layer 31 k' into RAM\n",
            "Loading 'layer 31 v' into RAM\n",
            "Saving converted checkpoint to checkpoints/mistralai/Mistral-7B-v0.1\n"
          ]
        }
      ],
      "source": [
        "!litgpt download --repo_id mistralai/Mistral-7B-v0.1\n",
        "# !litgpt convert_hf_checkpoint --checkpoint_dir checkpoints/mistralai/Mistral-7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qtl4xJePf-40",
        "outputId": "24dbc686-e603-42ca-de2d-c0eff31f2a6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model 'checkpoints/mistralai/Mistral-7B-v0.1/lit_model.pth' with {'name': 'Mistral-7B-v0.1', 'hf_config': {'name': 'Mistral-7B-v0.1', 'org': 'mistralai'}, 'scale_embeddings': False, 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 512, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'head_size': 128, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 8, 'shared_attention_norm': False, 'norm_class_name': 'RMSNorm', 'norm_eps': 1e-05, 'mlp_class_name': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 14336, 'rope_condense_ratio': 1, 'rope_base': 10000, 'n_expert': 0, 'n_expert_per_token': 0, 'rope_n_elem': 128}\n",
            "Time to instantiate model: 0.32 seconds.\n",
            "Time to load the model weights: 12.37 seconds.\n",
            "Seed set to 1234\n",
            "Are you good at answering Questions? Have you ever wondered how you would do in a Q&A? In this post, I will give you 10 Q&A Questions for you to practice with and if you get stuck, don’t worry, I have posted the answers\n",
            "Time for inference 1: 3.34 sec total, 14.99 tokens/sec\n",
            "Memory used: 14.52 GB\n"
          ]
        }
      ],
      "source": [
        "!litgpt generate base --precision bf16-true --checkpoint_dir checkpoints/mistralai/Mistral-7B-v0.1 --prompt \"Are you good at answering Questions?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RdeeuN9pfmn"
      },
      "source": [
        "## Get Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzV_1VpFpiQ5"
      },
      "source": [
        "## Finetune Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuwLFkdqpj-v",
        "outputId": "b8ddf064-56fc-44ab-94c5-72d652f55656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'checkpoint_dir': PosixPath('checkpoints/mistralai/Mistral-7B-v0.1'),\n",
            " 'data': AlpacaGPT4(mask_prompt=False,\n",
            "                    val_split_fraction=0.03847,\n",
            "                    prompt_style=<litgpt.prompts.Alpaca object at 0x79a72fa71a80>,\n",
            "                    ignore_index=-100,\n",
            "                    seed=42,\n",
            "                    num_workers=4,\n",
            "                    download_dir=PosixPath('data/alpacagpt4')),\n",
            " 'devices': 1,\n",
            " 'eval': EvalArgs(interval=100, max_new_tokens=100, max_iters=100),\n",
            " 'logger_name': 'csv',\n",
            " 'lora_alpha': 16,\n",
            " 'lora_dropout': 0.05,\n",
            " 'lora_head': False,\n",
            " 'lora_key': False,\n",
            " 'lora_mlp': False,\n",
            " 'lora_projection': False,\n",
            " 'lora_query': True,\n",
            " 'lora_r': 8,\n",
            " 'lora_value': True,\n",
            " 'out_dir': PosixPath('out/lora_weights/mistral7B-finetuned-test'),\n",
            " 'precision': 'bf16-true',\n",
            " 'quantize': 'bnb.fp4',\n",
            " 'seed': 1337,\n",
            " 'train': TrainArgs(save_interval=1,\n",
            "                    log_interval=16,\n",
            "                    global_batch_size=32,\n",
            "                    micro_batch_size=2,\n",
            "                    lr_warmup_steps=100,\n",
            "                    lr_warmup_fraction=None,\n",
            "                    epochs=3,\n",
            "                    max_tokens=None,\n",
            "                    max_steps=None,\n",
            "                    max_seq_length=512,\n",
            "                    tie_embeddings=None,\n",
            "                    learning_rate=0.0003,\n",
            "                    weight_decay=0.02,\n",
            "                    beta1=0.9,\n",
            "                    beta2=0.95,\n",
            "                    max_norm=None,\n",
            "                    min_lr=6e-05)}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 3,407,872\n",
            "Number of non-trainable parameters: 7,241,732,096\n",
            "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096\n",
            "Validating ...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:\n",
            "\n",
            "The movie recommendation that I think would be perfect for you to watch during the weekend is _The Truman Show_ (1998), directed by Peter Weir. This movie tells the story of a man, Truman Burbank, who lives a seemingly normal life in a small fishing village, not knowing that the whole world is a television show. The movie portrays a fascinating story about the nature of reality and the idea of individuality.\n",
            "\n",
            "What makes this movie\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/throughput.py:587: 'Tesla V100-SXM2-16GB' does not support torch.bfloat16\n",
            "Missing logger folder: out/lora_weights/mistral7B-finetuned-test/logs/csv\n",
            "Epoch 1 | iter 16 step 1 | loss train: 1.511, val: n/a | iter time: 1160.09 ms (step)\n",
            "Saving LoRA weights to 'out/lora_weights/mistral7B-finetuned-test/step-000001/lit_model.pth.lora'\n",
            "Epoch 1 | iter 32 step 2 | loss train: 1.699, val: n/a | iter time: 1970.00 ms (step)\n",
            "Saving LoRA weights to 'out/lora_weights/mistral7B-finetuned-test/step-000002/lit_model.pth.lora'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/litgpt\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litgpt/__main__.py\", line 131, in main\n",
            "    fn(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litgpt/finetune/lora.py\", line 140, in setup\n",
            "    fabric.launch(main, devices, seed, config, data, checkpoint_dir, out_dir, train, eval)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 866, in launch\n",
            "    return self._wrap_and_launch(function, self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 952, in _wrap_and_launch\n",
            "    return to_run(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 957, in _wrap_with_setup\n",
            "    return to_run(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litgpt/finetune/lora.py\", line 193, in main\n",
            "    fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litgpt/finetune/lora.py\", line 271, in fit\n",
            "    fabric.backward(loss / train.gradient_accumulation_iters(devices))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\", line 448, in backward\n",
            "    self._strategy.backward(tensor, module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/strategies/strategy.py\", line 191, in backward\n",
            "    self.precision.backward(tensor, module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning/fabric/plugins/precision/precision.py\", line 107, in backward\n",
            "    tensor.backward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 522, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 266, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!litgpt finetune lora \\\n",
        "  --checkpoint_dir \"checkpoints/mistralai/Mistral-7B-v0.1/\" \\\n",
        "  --precision bf16-true \\\n",
        "  --quantize bnb.fp4 \\\n",
        "  --data AlpacaGPT4\\\n",
        "  --out_dir \"out/lora_weights/mistral7B-finetuned-test/\" \\\n",
        "  --train.log_interval 16 \\\n",
        "  --train.save_interval 1 \\\n",
        "  --train.micro_batch_size 2 \\\n",
        "  --train.global_batch_size 32 \\\n",
        "  --train.max_seq_length 512 \\\n",
        "  --train.epochs 3 \\\n",
        "  --train.lr_warmup_steps=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cxTYKwbi87L",
        "outputId": "15be555d-55e7-4c1e-e44f-5b07b5433a21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model 'checkpoints/instruct-tuned/lit_model.pth' with {'name': 'Mistral-7B-v0.1', 'hf_config': {'name': 'Mistral-7B-v0.1', 'org': 'mistralai'}, 'scale_embeddings': False, 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 512, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'head_size': 128, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 8, 'shared_attention_norm': False, 'norm_class_name': 'RMSNorm', 'norm_eps': 1e-05, 'mlp_class_name': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 14336, 'rope_condense_ratio': 1, 'rope_base': 10000, 'n_expert': 0, 'n_expert_per_token': 0, 'rope_n_elem': 128}\n",
            "Time to instantiate model: 0.15 seconds.\n",
            "Time to load the model weights: 12.26 seconds.\n",
            "Seed set to 1234\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "fCHUNK: FLEXIBILITY AND OPTIONS 4+1: BSE+Master's Option for Duke Undergraduates Duke undergraduate students can complete undergrad and this master's degree in just five (5) years. Scholarship opportunity: The AI 4+1 BSE+Master's scholarship covers 20 percent of the costs. Eligibility and other conditions apply. MD+MEng in Artificial Intelligence Dual Degree Medical students at Duke can complete this degree during their third year. See Duke MEDx website and School of Medicine bulletin for details.Scholarship opportunity: The MD+MEng AI scholarship covers 20 percent of the costs. Eligibility and other conditions apply.=== CURRICULUM SCHEDULES The core of the curriculum follows a cohort-based course sequence On-Campus Accelerated Option: 12 MonthsSummer:Pre-requisite AIPI 503: Python & Data Science Math Bootcamp Fall: AIPI 510: Sourcing Data for Analytics AIPI 520: Modeling Process & Algorithms AIPI Departmental Elective MENG 570: Business Fundamentals for Engineers AIPI 501: Industry Seminar Series EGR 590: Career Strategy & Design ====== prompt: Is is possible to complete the Meng AI program in just 1 more year after undergrad and what are prerequisites?\n",
            "\n",
            "### Response:\n",
            "Sure, it is possible to complete the MEng in AI program in just one more year after undergrad. The prerequisites for the program include a solid background in Computer Science/Engineering and Mathematics, two or more years of calcul\n",
            "Time for inference 1: 2.95 sec total, 16.98 tokens/sec\n",
            "Memory used: 14.79 GB\n"
          ]
        }
      ],
      "source": [
        "!litgpt generate base \\\n",
        "  --checkpoint_dir \"checkpoints/instruct-tuned\" \\\n",
        "  --prompt f\"{prompt_RAG}\" \\\n",
        "  --precision bf16-true"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_RAG = \"CHUNK: FLEXIBILITY AND OPTIONS \\\n",
        "4+1: BSE+Master's Option for Duke Undergraduates \\\n",
        "Duke undergraduate students can complete undergrad and this master's degree in just five (5) years. \\\n",
        "Scholarship opportunity: The AI 4+1 BSE+Master's scholarship covers 20 percent of the costs. Eligibility and other conditions apply. \\\n",
        "MD+MEng in Artificial Intelligence Dual Degree \\\n",
        "Medical students at Duke can complete this degree during their third year. See Duke MEDx website and School of Medicine bulletin for details.\\\n",
        "Scholarship opportunity: The MD+MEng AI scholarship covers 20 percent of the costs. Eligibility and other conditions apply.\\\n",
        "=== \\\n",
        "CURRICULUM SCHEDULES \\\n",
        "The core of the curriculum follows a cohort-based course sequence \\\n",
        "On-Campus Accelerated Option: 12 Months\\\n",
        "Summer:\\\n",
        "Pre-requisite \\\n",
        "AIPI 503: Python & Data Science Math Bootcamp \\\n",
        "Fall: \\\n",
        "AIPI 510: Sourcing Data for Analytics \\\n",
        "AIPI 520: Modeling Process & Algorithms \\\n",
        "AIPI Departmental Elective \\\n",
        "MENG 570: Business Fundamentals for Engineers \\\n",
        "AIPI 501: Industry Seminar Series \\\n",
        "EGR 590: Career Strategy & Design \\\n",
        "====== \\\n",
        "prompt: Is is possible to complete the Meng AI program in just 1 more year after undergrad and what are prerequisites?\""
      ],
      "metadata": {
        "id": "VP2uWdktGuhB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spmrBqQGi3_7"
      },
      "source": [
        "### Merging LoRA Weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.mkdir(\"checkpoints/instruct-tuned\")\n",
        "os.mkdir(\"checkpoints/instruct-tuned-hf\")"
      ],
      "metadata": {
        "id": "ujgiy1kiL-E8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip drive/MyDrive/instruct-alpaca.zip -d checkpoints/instruct-tuned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuPifflwEdnL",
        "outputId": "ace2b555-4d53-48f0-d8ee-da4b25180387"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drive/MyDrive/instruct-alpaca.zip\n",
            "  inflating: checkpoints/instruct-tuned/lit_model.pth.lora  \n",
            "  inflating: checkpoints/instruct-tuned/tokenizer_config.json  \n",
            "  inflating: checkpoints/instruct-tuned/model_config.yaml  \n",
            "  inflating: checkpoints/instruct-tuned/tokenizer.json  \n",
            "  inflating: checkpoints/instruct-tuned/generation_config.json  \n",
            "  inflating: checkpoints/instruct-tuned/hyperparameters.yaml  \n",
            " extracting: checkpoints/instruct-tuned/prompt_style.yaml  \n",
            "  inflating: checkpoints/instruct-tuned/tokenizer.model  \n",
            "   creating: checkpoints/instruct-tuned/.ipynb_checkpoints/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOuhMfoQjaZr",
        "outputId": "83b9f63a-c3da-4924-89fa-6b7f2c3f8c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved merged weights to 'checkpoints/instruct-tuned/lit_model.pth'\n"
          ]
        }
      ],
      "source": [
        "!litgpt merge_lora \\\n",
        " --checkpoint_dir \"checkpoints/instruct-tuned\" \\\n",
        " --pretrained_checkpoint_dir \"checkpoints/mistralai/Mistral-7B-v0.1\" \\\n",
        " --precision bf16-true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "revbUaVn6Mo_"
      },
      "source": [
        "### Zip and Save"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/checkpoints/\")\n",
        "!zip -r Mistral-instruct-alpaca-400.zip Mistral-instruct-alpaca-400"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXS0xg9JXgLU",
        "outputId": "09dc70c3-e616-43ac-9640-22e4155c4b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: Mistral-instruct-alpaca-400/ (stored 0%)\n",
            "  adding: Mistral-instruct-alpaca-400/lit_model.pth.lora (deflated 21%)\n",
            "  adding: Mistral-instruct-alpaca-400/lit_model.pth\n",
            "\n",
            "\n",
            "zip error: Interrupted (aborting)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl3rrAc5plIa",
        "outputId": "d399b87c-641b-48f2-bdc0-04aa3b34d218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: out/lora_weights/mistral7B-finetuned-test/step-000001/ (stored 0%)\n",
            "  adding: out/lora_weights/mistral7B-finetuned-test/step-000001/lit_model.pth.lora (deflated 52%)\n",
            "  adding: out/lora_weights/mistral7B-finetuned-test/step-000001/tokenizer_config.json (deflated 68%)\n",
            "  adding: out/lora_weights/mistral7B-finetuned-test/step-000001/model_config.yaml (deflated 42%)\n",
            "  adding: out/lora_weights/mistral7B-finetuned-test/step-000001/tokenizer.json (deflated 74%)\n",
            "  adding: out/lora_weights/mistral7B-finetuned-test/step-000001/generation_config.json (deflated 22%)\n",
            "  adding: out/lora_weights/mistral7B-finetuned-test/step-000001/hyperparameters.yaml (deflated 46%)\n",
            "  adding: out/lora_weights/mistral7B-finetuned-test/step-000001/prompt_style.yaml (stored 0%)\n",
            "  adding: out/lora_weights/mistral7B-finetuned-test/step-000001/tokenizer.model (deflated 55%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r mistral7B-finetuned-alpaca-final.zip out/lora_weights/mistral7B-finetuned-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPWwL7kX1_8Y",
        "outputId": "48f19b24-bd40-4c23-bf22-73b1d3ee159b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'mistral7B-finetuned-logic-final.zip': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cp mistral7B-finetuned-alpaca-final.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip instruct-alpaca.zip -d checkpoints/Mistral-instruct-alpaca"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AarTN9-hQuz0",
        "outputId": "b5e84de7-85e8-422a-b0bf-fafb2052ccd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  instruct-alpaca.zip\n",
            "  inflating: checkpoints/Mistral-instruct-alpaca/lit_model.pth.lora  \n",
            "  inflating: checkpoints/Mistral-instruct-alpaca/tokenizer_config.json  \n",
            "  inflating: checkpoints/Mistral-instruct-alpaca/model_config.yaml  \n",
            "  inflating: checkpoints/Mistral-instruct-alpaca/tokenizer.json  \n",
            "  inflating: checkpoints/Mistral-instruct-alpaca/generation_config.json  \n",
            "  inflating: checkpoints/Mistral-instruct-alpaca/hyperparameters.yaml  \n",
            " extracting: checkpoints/Mistral-instruct-alpaca/prompt_style.yaml  \n",
            "  inflating: checkpoints/Mistral-instruct-alpaca/tokenizer.model  \n",
            "   creating: checkpoints/Mistral-instruct-alpaca/.ipynb_checkpoints/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ9UzYWn_ETX"
      },
      "source": [
        "### Convert to Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!litgpt convert from_litgpt \\\n",
        "    --checkpoint_dir checkpoints/instruct-tuned \\\n",
        "    --output_dir checkpoints/instruct-tuned-hf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPU8zF9s_TTC",
        "outputId": "0860145d-a2d2-4e9c-bc44-b7c1007bbbe2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 'lm_head.weight' into RAM\n",
            "Loading 'transformer.wte.weight' into RAM\n",
            "Loading 'transformer.h.0.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.0.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.0.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.0.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.0.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.0.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.0.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.1.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.1.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.1.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.1.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.1.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.1.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.1.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.2.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.2.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.2.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.2.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.2.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.2.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.2.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.3.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.3.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.3.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.3.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.3.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.3.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.3.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.4.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.4.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.4.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.4.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.4.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.4.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.4.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.5.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.5.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.5.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.5.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.5.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.5.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.5.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.6.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.6.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.6.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.6.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.6.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.6.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.6.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.7.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.7.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.7.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.7.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.7.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.7.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.7.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.8.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.8.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.8.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.8.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.8.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.8.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.8.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.9.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.9.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.9.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.9.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.9.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.9.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.9.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.10.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.10.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.10.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.10.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.10.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.10.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.10.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.11.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.11.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.11.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.11.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.11.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.11.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.11.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.12.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.12.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.12.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.12.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.12.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.12.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.12.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.13.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.13.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.13.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.13.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.13.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.13.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.13.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.14.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.14.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.14.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.14.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.14.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.14.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.14.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.15.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.15.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.15.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.15.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.15.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.15.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.15.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.16.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.16.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.16.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.16.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.16.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.16.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.16.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.17.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.17.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.17.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.17.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.17.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.17.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.17.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.18.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.18.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.18.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.18.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.18.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.18.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.18.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.19.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.19.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.19.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.19.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.19.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.19.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.19.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.20.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.20.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.20.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.20.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.20.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.20.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.20.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.21.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.21.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.21.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.21.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.21.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.21.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.21.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.22.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.22.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.22.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.22.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.22.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.22.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.22.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.23.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.23.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.23.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.23.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.23.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.23.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.23.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.24.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.24.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.24.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.24.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.24.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.24.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.24.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.25.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.25.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.25.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.25.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.25.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.25.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.25.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.26.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.26.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.26.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.26.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.26.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.26.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.26.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.27.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.27.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.27.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.27.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.27.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.27.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.27.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.28.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.28.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.28.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.28.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.28.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.28.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.28.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.29.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.29.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.29.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.29.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.29.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.29.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.29.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.30.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.30.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.30.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.30.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.30.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.30.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.30.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.h.31.norm_1.weight' into RAM\n",
            "Loading 'transformer.h.31.attn.attn.weight' into RAM\n",
            "Loading 'transformer.h.31.attn.proj.weight' into RAM\n",
            "Loading 'transformer.h.31.norm_2.weight' into RAM\n",
            "Loading 'transformer.h.31.mlp.fc_1.weight' into RAM\n",
            "Loading 'transformer.h.31.mlp.fc_2.weight' into RAM\n",
            "Loading 'transformer.h.31.mlp.proj.weight' into RAM\n",
            "Loading 'transformer.ln_f.weight' into RAM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "fd51472d005b4c3da02f16f37ccb7791",
            "6acdbd48923c40b9aeadc51cd7aeb1fc",
            "33e354a4544345a7b00b87f644616666",
            "04bf54d64e4e48b59917d5f0ec1e411d",
            "f71c95eaaacd47a08c357d036c2c36f1",
            "c23f7df70c954da69486426fb7454022",
            "02b9ed567c6c4cf898df652f539d83bb",
            "1fb31398832d41aca83092ba39b43e9b",
            "da08d938885848f9b67ec7e9df6e08e7",
            "dfce6248921e4dc39f8a52e92d7b259f",
            "b59a9c4254204d89bd5b53c0b65a1828",
            "0fc528552d39417bb388fca91ea9ad09",
            "1fd6fd7fe7044fa8916938d5a5204db5",
            "94c3a8c3adfa4e4e979e7645c8f989af",
            "03f1b113abc649d68649a7e16bed921b",
            "0e87f167b87242b9ab8b1c22489ccdfb",
            "1b17567e9d9941699a3d1d9bae94fca1",
            "54f7b4e9f58644ae9656cba190467bf8",
            "bae15867fd9044b5a1efd2dd08768299",
            "42a3911d5e8940cb9c9eb8bad765a27a",
            "7ee67645617b424ab79a7060e1c98d04",
            "0b3a6450ec5c4370ae73a6026637a01e",
            "1d1cb913b30c4c458afeded4bcc1991d",
            "8761e021159c4d389b230352bb5a0dba",
            "5d4d15fa0baa4c8d8ece7acaac6ba742",
            "b62cd7e637574dafbb3de952af171856",
            "8ebfbdcf003b48c6a4dc61b05c590231",
            "d1fd601c97f94c83815bd75d7878f6a6",
            "7b00fcad92ad42ce961b47ea6d684b70",
            "1881b90dad3e456b916aedb830e7f200",
            "8660b35ead3445f3a8fd8a4446eb463b",
            "82ec6108bb384c1fa46b647771433b7d"
          ]
        },
        "id": "fo71lNIdK3g3",
        "outputId": "be987cfc-b607-4fc2-d646-61ee0ef072f2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd51472d005b4c3da02f16f37ccb7791"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel\n",
        "\n",
        "state_dict = torch.load('checkpoints/instruct-tuned-hf/model.pth')\n",
        "model = AutoModel.from_pretrained(\"mistralai/Mistral-7B-v0.1\", state_dict=state_dict)"
      ],
      "metadata": {
        "id": "XB3w1dTjI3Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"checkpoints/custom-instruct-mistral7B\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "FSLcdoF8LAYe",
        "outputId": "8ac33122-c378-4095-fc3c-1ed66551eaf9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e976d680909e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpoints/custom-instruct-mistral7B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"mkeohane01/mistral-instruct-duke590\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "tFOR6hoITtHN",
        "outputId": "c52f30a6-9b3e-4fba-e059-aa1975468795"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4bd25e98f068>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mkeohane01/mistral-instruct-duke590\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PushToHubCallback\n",
        "\n",
        "push_to_hub_callback = PushToHubCallback(\n",
        "    output_dir=\"checkpoints/custom-instruct-mistral7B\", tokenizer=tokenizer, hub_model_id=\"mkeohane01/mistral-instruct-duke590\"\n",
        ")"
      ],
      "metadata": {
        "id": "sYPUZTYlLPMT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1UgHFY8v39Uf5uO9ROv3PgPMkG6w8vh-R",
      "authorship_tag": "ABX9TyNUvRuLX0/ximRo0Vovjgd8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fd51472d005b4c3da02f16f37ccb7791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ee67645617b424ab79a7060e1c98d04",
              "IPY_MODEL_0b3a6450ec5c4370ae73a6026637a01e",
              "IPY_MODEL_1d1cb913b30c4c458afeded4bcc1991d",
              "IPY_MODEL_8761e021159c4d389b230352bb5a0dba"
            ],
            "layout": "IPY_MODEL_02b9ed567c6c4cf898df652f539d83bb"
          }
        },
        "6acdbd48923c40b9aeadc51cd7aeb1fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fb31398832d41aca83092ba39b43e9b",
            "placeholder": "​",
            "style": "IPY_MODEL_da08d938885848f9b67ec7e9df6e08e7",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "33e354a4544345a7b00b87f644616666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_dfce6248921e4dc39f8a52e92d7b259f",
            "placeholder": "​",
            "style": "IPY_MODEL_b59a9c4254204d89bd5b53c0b65a1828",
            "value": ""
          }
        },
        "04bf54d64e4e48b59917d5f0ec1e411d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_0fc528552d39417bb388fca91ea9ad09",
            "style": "IPY_MODEL_1fd6fd7fe7044fa8916938d5a5204db5",
            "value": true
          }
        },
        "f71c95eaaacd47a08c357d036c2c36f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_94c3a8c3adfa4e4e979e7645c8f989af",
            "style": "IPY_MODEL_03f1b113abc649d68649a7e16bed921b",
            "tooltip": ""
          }
        },
        "c23f7df70c954da69486426fb7454022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e87f167b87242b9ab8b1c22489ccdfb",
            "placeholder": "​",
            "style": "IPY_MODEL_1b17567e9d9941699a3d1d9bae94fca1",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "02b9ed567c6c4cf898df652f539d83bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "1fb31398832d41aca83092ba39b43e9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da08d938885848f9b67ec7e9df6e08e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfce6248921e4dc39f8a52e92d7b259f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b59a9c4254204d89bd5b53c0b65a1828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fc528552d39417bb388fca91ea9ad09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fd6fd7fe7044fa8916938d5a5204db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94c3a8c3adfa4e4e979e7645c8f989af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03f1b113abc649d68649a7e16bed921b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0e87f167b87242b9ab8b1c22489ccdfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b17567e9d9941699a3d1d9bae94fca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54f7b4e9f58644ae9656cba190467bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bae15867fd9044b5a1efd2dd08768299",
            "placeholder": "​",
            "style": "IPY_MODEL_42a3911d5e8940cb9c9eb8bad765a27a",
            "value": "Connecting..."
          }
        },
        "bae15867fd9044b5a1efd2dd08768299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42a3911d5e8940cb9c9eb8bad765a27a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ee67645617b424ab79a7060e1c98d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d4d15fa0baa4c8d8ece7acaac6ba742",
            "placeholder": "​",
            "style": "IPY_MODEL_b62cd7e637574dafbb3de952af171856",
            "value": "Token is valid (permission: write)."
          }
        },
        "0b3a6450ec5c4370ae73a6026637a01e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ebfbdcf003b48c6a4dc61b05c590231",
            "placeholder": "​",
            "style": "IPY_MODEL_d1fd601c97f94c83815bd75d7878f6a6",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "1d1cb913b30c4c458afeded4bcc1991d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b00fcad92ad42ce961b47ea6d684b70",
            "placeholder": "​",
            "style": "IPY_MODEL_1881b90dad3e456b916aedb830e7f200",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "8761e021159c4d389b230352bb5a0dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8660b35ead3445f3a8fd8a4446eb463b",
            "placeholder": "​",
            "style": "IPY_MODEL_82ec6108bb384c1fa46b647771433b7d",
            "value": "Login successful"
          }
        },
        "5d4d15fa0baa4c8d8ece7acaac6ba742": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b62cd7e637574dafbb3de952af171856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ebfbdcf003b48c6a4dc61b05c590231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1fd601c97f94c83815bd75d7878f6a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b00fcad92ad42ce961b47ea6d684b70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1881b90dad3e456b916aedb830e7f200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8660b35ead3445f3a8fd8a4446eb463b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82ec6108bb384c1fa46b647771433b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}